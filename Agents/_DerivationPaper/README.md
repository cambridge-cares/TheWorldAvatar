# Description

This repository contains several (helper) scripts to support the finalisation of a MVP of the King'sLynn use case required for the Derivation Paper. This is **NOT** an agent; however, multiple parts of the scripts developed here will be migrated into agents actually used in the (not simplified/mocked) King's Lynn use case.

The details provided here are mainly for documentation of the overall workflow to enable the use case MVP and ensure reproducibility.


&nbsp;
# Requirements

- The following scripts have been tested with Python >3.9
- To use `py4jps` one also needs [Java 11] installed
- As this instantiation uses Blazegraph and PostgreSQL running in Docker containers, you need to have Docker installed on your machine. Details on how to set up a [Docker environment] can be found in the TWA wiki. Furthermore, access to the [CMCL Docker image registry] is required.

## Installation of required packages

It is highly recommended to use a [virtual environment], which can be created as follows:

`(Windows)`
```cmd
$ python -m venv deriv_venv
$ deriv_venv\Scripts\activate.bat
(deriv_venv) $
```

To install required packages, run the following command:

```bash
# build and install
(deriv_venv) $ python -m pip install -r requirements.txt
```

## Spinning up Docker Stack

A [docker-compose_stack.yml] file is provided to spin up a stack with a Blazegraph and a PostgreSQL container. Both PostgreSQL and Blazegraph use volumes to ensure data persistence. To spin up the stack, run the following command from the same directory where this README is located:
```bash
# Spin up container stack
docker-compose -f "docker-compose_stack.yml" up -d
```

Another [docker-compose-agents.yml] file is provided to spin up a stack with the agents involved in this MVP. To spin up the stack, run the following command from the same directory where this README is located:
```bash
# Spin up container stack
docker-compose -f "docker-compose-agents.yml" up -d
```
Or, if you use VS Code, you can right click the docker compose file and select `Compose Up - Select Services` to spin up a specific agent.

> **NOTE** The agents' image mentioned in this docker compose file are mocked version for this mvp that work without using the stack-manager. They are published in `ghcr.io/cambridge-cares`, you may need GitHub personal access token to pull the images for the first time.

> **NOTE** All agents in the `docker-compose-agents.yml` are configured with `REGISTER_AGENT=true`, so one need to make sure the blazegraph container (in [docker-compose_stack.yml]) is ready to accept SPARQL query/update before spinning up the agents.


&nbsp;
# Workflow

## 0. Irrgularities in data to double check before uploading data and derivation markup
1. Each instantiated `obe:Property` has **ONLY ONE** instance of `obe:TransactionRecord` associated with it via `obe:hasLatestTransactionRecord`. In situation of multiple instances attached, the duplicated ones or the earlier ones should be deleted to keep only the latest one. Those have more than one instance can be filtered out by:
    ```sparql
    PREFIX obe: <https://www.theworldavatar.com/kg/ontobuiltenv/>
    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    SELECT ?property (count(?tx) as ?numOfTx)
    WHERE {
        ?property rdf:type/rdfs:subClassOf* obe:Property.
        ?property obe:hasLatestTransactionRecord ?tx.
    }
    GROUP BY ?property
    HAVING (?numOfTx > 1)
    ```
2. As there exists hierarchical relationship `obe:isIn` between a flat/apartment and its parent building it is contained within, the parent building should also be marked as `affected` if any of its child flat/apartment is affected by the flood. To check if this is valid for the entire triple store, one may perfom below query, where the `label` and `label_parent_property` should also give same result as `affected` for the mock dataset. However, note that for the proper use case this might not be necessary as the building information will be retrieved from ontop.
    ```sparql
    PREFIX obe: <https://www.theworldavatar.com/kg/ontobuiltenv/>
    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    SELECT ?property ?label ?parent_property ?label_parent_property
    WHERE {
        ?property rdf:type/rdfs:subClassOf* obe:Property.
        ?property rdfs:label ?label.
        ?property obe:isIn ?parent_property.
        OPTIONAL {?parent_property rdfs:label ?label_parent_property.}
    }
    ```
    > **Known limitation** Those parent buildings (if not instantiated from EPC, but only generated by agent based on flats) do not have a location information as those are not provided from API. For now we will mainly focus on buildings with location. If a building does not have a location, it cannot be detected as affected by QGIS/PostGIS, as there is no way to query whether it's within the polygon or not.

## 1. Data Preparation

Upload previously instantiated properties required for the use case. Either download the [consolidated and labeled triples] and place them into the [data] folder before running [data_preparation.py] as main script or follow steps 1.1. - 1.4 below.

> **NOTE** As of the time of writing, there exists 622,050 triples in the [consolidated and labeled triples] file which was used to as the basis for the derivation agents' operations. These triples include the symbol of pound sterling, as well as the labels for affected buildings. The triples about `PropertyPriceIndex` and its corresponding time series triples are not included in this file. After one placed this file into the [data] folder, running [data_preparation.py] as main script will upload this file to the triple store and instantiate further 6 triples detailed in [2. Initialise Property Price Index](#2-initialise-property-price-index) (done by `initialise_ukhpi(kg_client)`).

## 1.1 Consolidate previously exported triples

This minimum demonstration example is based on two previously exported sets of triples: One instantiation of properties (i.e. buildings and flats) includes their geospatial location (as points) and the other contains previous sales transactions (if available). Both files can be found in the [kg_data folder] on Dropbox. The data in both files have been consolidated by matching properties based on their identifiers, which matches for buildings with available EPC, and hence address and sales transaction data. The [consolidated triples] (as the time of writing, this file should contain 621,759 triples) can also be found on Dropbox and the SPARQL `matching_query` is provided in the [resources] folder (for reference). **Please note** that the variable `triples_file` in [data_preparation.py] needs to be updated accordingly.

## 1.2 Instantiate consolidated triples

Before starting the instantiation, ensure that the properties in [configs.py] match the settings in the `docker-compose_stack.yml` file. Then download the [consolidated triples] file and place it into the [data] folder (filename to be specified by variable `triples_file` in [data_preparation.py]).

Then simply run [data_preparation.py] as main script. **Please note** that this also add triples to represent the pound sterling symbol properly.

## 1.3 Identify buildings within flood polygon

QGIS is used to identify buildings within the flood warning polygon of interest. The QGIS project file as well as the required (geospatial) input files can be found in the [geospatial_analysis folder] on Dropbox:
- `flood-areas.geojson`: one flood warning polygon as GeoJSON file (same as version-controlled version in [data] folder here) 
- `building_locations.csv`: point locations of instantiated properties as created by `extract_property_locations` method in [data_preparation.py]
- `affected_property_iris.csv`: one-column csv file of all buildings within the flood warning polygon (same as version-controlled version in [data] folder here) 

Rough QGIS workflow to identify buildings within flood polygon and create `affected_property_iris.csv`:

1. Add `flood-areas.geojson` and `building_locations.csv` as vector layers
2. Create points from csv by (settings to use: 'EPSG:4326', 'XFIELD' : 'longitude', 'YFIELD' : 'latitude', 'ZFIELD' : ''):
    > Processing Toolbox > Create points layer from table >
3. Identify properties within flood polygon by:
    > Processing Toolbox > Extract by location
4. Export affected properties as csv

## 1.4 Label affected properties in Blazegraph

There are 289 buildings being affected by the (hypothetical) flood event. Running the `attach_labels` function in [data_preparation.py] will attach an `affected` label (specified in `data_preparation.py`) to all property IRIs listed in the [affected_property_iris] csv file. This will be used to mock the geospatial queries to obtain buildings within a flood polygon.

## 2. Initialise Property Price Index

Running the [data_preparation.py] module as main script also initialises the Property Price Index (PPI) in both KG and RDB, i.e. 6 triples similar to below will be added:
```
# Define PropertyPriceIndex_KingsLynn and the AdministrativeDistrict it represents for
<https://www.theworldavatar.com/kg/ontobuiltenv/PropertyPriceIndex_KingsLynn> rdf:type <https://www.theworldavatar.com/kg/ontobuiltenv/PropertyPriceIndex> .
<https://www.theworldavatar.com/kg/ontobuiltenv/PropertyPriceIndex_KingsLynn> <https://www.theworldavatar.com/kg/ontobuiltenv/representativeFor> <https://www.theworldavatar.com/kg/ontobuiltenv/AdministrativeDistrict_b240d043-4d79-4660-8005-3074ecb84176> .

# Time Series related triples for PropertyPriceIndex_KingsLynn
<https://www.theworldavatar.com/kg/ontobuiltenv/PropertyPriceIndex_KingsLynn> <https://www.theworldavatar.com/kg/ontotimeseries/hasTimeSeries> <https://www.theworldavatar.com/kg/ontotimeseries/Timeseries_9fe1165f-8863-48f9-80e3-199e8b37e474> .

<https://www.theworldavatar.com/kg/ontotimeseries/Timeseries_9fe1165f-8863-48f9-80e3-199e8b37e474> rdf:type <https://www.theworldavatar.com/kg/ontotimeseries/TimeSeries> .
<https://www.theworldavatar.com/kg/ontotimeseries/Timeseries_9fe1165f-8863-48f9-80e3-199e8b37e474> <https://www.theworldavatar.com/kg/ontotimeseries/hasRDB> "jdbc:postgresql://localhost:9997/postgres" .
<https://www.theworldavatar.com/kg/ontotimeseries/Timeseries_9fe1165f-8863-48f9-80e3-199e8b37e474> <https://www.theworldavatar.com/kg/ontotimeseries/hasTimeUnit> "YYYY-MM-DD" .
```

The initially uploaded PPI data only includes values until August 2022 (i.e. excluding the most recent month at time of writing, September 2022). To update the instantiated Property Price Index and hence trigger a second cascade of derivations, run [property_price_index.py] as main script.

## 3. Instantiate Flood Warning

Run the [flood_warning.py] module as main script to instantiate the flood warning (which affects the previously labeled buildings). Please note that only the absolute minimum relationships are instantiated, which are required for the Flood Assessment Agent to pick up the flood warning.

## 4. Create derivation markup to generate new information (first round of derivation cascade)
1. Create derivation markup for `AverageSquareMetrePrice Derivation` by executing the below command:
    ```bash
    python markup_avg_sqm_price.py
    ```
    This file queries all instantiated postal code and requests for derivation markup with the `createSyncDerivationForNewInfoWithHttpUrl` function to compute the average price on the spot for those postal code that **contains at least one building**. After running the script, 1,123 derivations will be generated, which may take ~30 minutes if none of the derivations exist before.
2. Create derivation markup for `PropertyValueEstimation Derivation` by executing the below command:
    ```bash
    python markup_property_value_est.py
    ```
    This file queries all instantiated properties but ONLY requests for derivation markup of the mock affected buildings as listed in [affected_property_iris]. It also uses the `createSyncDerivationForNewInfoWithHttpUrl` function and will generate 289 derivations once executed. This may take ~7 minutes if none of the derivations exist before.
3. Create derivation markup for `FloodAssessment Derivation` by executing the below command:
    ```bash
    python markup_flood_assessment.py
    ```
    This file queries all affected buildings to retrieve the IRI of itself and its market value previously instantiated by `PropertyValueEstimation Derivation`. It then creates an asynchronous derivation with `createAsyncDerivationForNewInfo` function. The computation of flood assessment will be picked up and executed shortly.

Up to this point, all new information should be generated for the flood assessment exercise. A [snapshot after the first round derivation] consists of total 721,228 triples can be downloaded from Dropbox.

## 5. Update PropertyPriceIndex
To demonstrate the capability of automated information update upon access, the value of the instantiated `PropertyPriceIndex` in September 2022 can be included in the time series. This can be done by executing the following command:
```bash
python property_price_index.py
```

## 6. Request for another round of update
Updating the `PropertyPriceIndex` in the last step makes the two derivations that directly derived from it outdated, i.e. `AverageSquareMetrePrice Derivation` and `PropertyValueEstimation Derivation`, which further makes the `FloodAssessment Derivation` outdated. To obtain the latest flood impact assessment, on can run `markup_flood_assessment.py` as the main script again. As the `FloodAssessment Derivation` instance is already instantiated, this script requests for an update (NOTE that the execution time for `derivation_client.unifiedUpdateDerivation(flood_assessment_derivation_iri)` can be some ~35 seconds due to the large amount of derivation inputs and total triples in the knowledge graph) and the rest will be handled by the `FloodAssessmentAgent`. In this process, `FloodAssessmentAgent` fires HTTP request to both `AverageSquareMetrePriceAgent` and `PropertyValueEstimationAgent` in sequence to obtain the latest property value and subsequently compute the latest flood impact. By design of the Derived Information Framework, the old instances will be replaced with the new information, resulting in that the number of triples remains unchanged once the second round of cascade is completed. A [snapshot after the second round derivation] consists of total 721,228 triples can be downloaded from Dropbox.


&nbsp;
# Digital Twin Visualisation Framework (DTVF)

The instantiated data is visualised using the Digital Twin Visualisation Framework ([DTVF]). The file structure is based on the [example Mapbox visualisation].

## Creating the Visualisation

Detailed instructions on how to create (and customise) the visualisation can be found in the [example Mapbox visualisation] and [DTVF] READMEs. To deploy the visualisation including all data as specified in the `data.json` file as Docker container, please run the following commands from within the [visualisation] directory:

```bash
# To build the Image:
docker-compose -p kings-lynn -f ./docker/docker-compose.yml build --force-rm
# To generate a Container (i.e. run the Image):
docker-compose -p kings-lynn -f ./docker/docker-compose.yml up -d --force-recreate

# To rebuild the image and deploy the container
bash ./redeploy.sh
```

**Please note**: 
1) A valid Mapbox API username and token must be provided in your `index.html` file.
2) All required data files for visualisation should be available. This requires the docker stack of knowledge graph to be up and running. Also one need to place the two snapshot files ([snapshot after the first round derivation] and [snapshot after the second round derivation]) in the [data] folder. Once these requirements are satisfied, one can simply run the [generate_geojson] script to create the required geojson files. **Please note**: To enhance the visualisation, [generate_geojson] calls a [amend_geojsons] script to post-process the generated geojson files, i.e. ensures consistent plotting sequence of buildings between derivation layers and improved scaling of property value estimates. 

Once started successfully, the visualisation will be available at `http://localhost:80`.


&nbsp;
# To be improved

## Issues
1. Length of the request line exceeds the limit of gunicorn server when `FloodAssessmentAgent` requesting `AverageSquareMetreAgent` and `PropertyValueEstimationAgent` for synchronous derivation update via HTTP GET request, e.g. `Request Line is too large (4807 &gt; 4094)`

    **Workaround:** add `"--limit-request-line", "0"` (unlimited request line length) to the list of `entrypoint` of relevant agents in [docker-compose-agents.yml], see https://docs.gunicorn.org/en/stable/settings.html#limit-request-line **NOTE This should be used with caution especially in production image**

    **Potential long-term solution:** change request for updating synchronous derivation to HTTP POST

2. Unstable passing of Java object to the Python side when handling synchronous derivation request whereas the monitoring asynchronous derivation is running as a periodical job

    **Workaround:** disable monitoring async derivations by shutting down the scheduler, i.e. line `RUN sed -i "/agent.start_all_periodical_job()/a \ \ \ \ agent.scheduler.shutdown()" ./avgsqmpriceagent/entry_point.py` added in the target `derivation_mvp` in the `Dockerfile` of both `AverageSquareMetreAgent` and `PropertyValueEstimationAgent`

    **Potential long-term solution:** investigate threading between Python and Java communications in `py4j`, might be relevant https://www.py4j.org/faq.html#is-py4j-thread-safe

## Nice-to-have
1. Better error handling for updating synchronous derivations, e.g. ideally automate re-firing the HTTP request when running into exceptions
2. Add [wait-for-it] script to the derivation agent docker container so that one don't need to worry about waiting for the triple store to be ready before spinning up agent containers - this might be already be fixed if the agent is deployed by stack-manager, to be confirmed
3. Remove manually conversion between `host.docker.internal` and `localhost` for the agents' HTTP endpoint - this could potentially be solved by using stack-manager to route the HTTP requests


&nbsp;
# Authors #
Markus Hofmeister (mh807@cam.ac.uk), November 2022

Jiaru Bai (jb2197@cam.ac.uk), December 2022


<!-- Links -->
[Java 11]: https://adoptium.net/en-GB/temurin/releases/?version=11
[virtual environment]: https://docs.python.org/3/tutorial/venv.html
[Docker environment]: https://github.com/cambridge-cares/TheWorldAvatar/wiki/Docker%3A-Environment
[CMCL Docker image registry]: https://github.com/cambridge-cares/TheWorldAvatar/wiki/Docker%3A-Image-registry
[DTVF]: https://github.com/cambridge-cares/TheWorldAvatar/wiki/Digital-Twin-Visualisations
[example Mapbox visualisation]: https://github.com/cambridge-cares/TheWorldAvatar/tree/main/web/digital-twin-vis-framework/example-mapbox-vis
[wait-for-it]: https://github.com/vishnubob/wait-for-it

<!-- Data -->
[kg_data folder]: https://www.dropbox.com/home/CoMo%20shared/mh807/DerivationPaper/kg_data
[geospatial_analysis folder]: https://www.dropbox.com/home/CoMo%20shared/mh807/DerivationPaper/geospatial_analysis
[consolidated triples]: https://www.dropbox.com/home/CoMo%20shared/mh807/DerivationPaper/kg_data?preview=20221130_consolidated_properties.nt
[consolidated and labeled triples]: https://www.dropbox.com/home/CoMo%20shared/mh807/DerivationPaper/kg_data?preview=20221203_consolidated_and_labeled_properties.nt
[snapshot after the first round derivation]: https://www.dropbox.com/home/CoMo%20shared/mh807/DerivationPaper/kg_data/snapshot?preview=20221204_snapshot_after_the_first_round_derivation.nt
[snapshot after the second round derivation]: https://www.dropbox.com/home/CoMo%20shared/mh807/DerivationPaper/kg_data/snapshot?preview=20221204_snapshot_after_the_second_round_derivation.nt

[resources]: resources
[configs.py]: configs.py
[data]: data
[data_preparation.py]: data_preparation.py
[property_price_index.py]: property_price_index.py
[flood_warning.py]: flood_warning.py
[affected_property_iris]: data/affected_property_iris.csv
[docker-compose_stack.yml]: docker-compose_stack.yml
[docker-compose-agents.yml]: docker-compose-agents.yml
[visualisation]: visualisation/
[generate_geojson]: generate_geojson.py
[amend_geojsons]: amend_geojsons.py
