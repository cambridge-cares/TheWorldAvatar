# Flood Agent
This agent downloads data from https://environment.data.gov.uk/flood-monitoring/doc/reference and stores them in Blazegraph (station info) and PostgreSQL (time series data).

## Building and running
This section specifies the minimum requirement to build the docker image. 

This agent uses the Maven repository at https://maven.pkg.github.com/cambridge-cares/TheWorldAvatar/ (in addition to Maven central).
You'll need to provide your credentials in single-word text files located like this:
```
credentials/
  repo_username.txt
  repo_password.txt
```

repo_username.txt should contain your github username, and repo_password.txt your github [personal access token](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token), which must have a 'scope' that [allows you to publish and install packages](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-apache-maven-registry#authenticating-to-github-packages).

This agent is designed to work with the stack spun up by stack-manager, to start a stack, navigate to `Deploy/stacks/dynamic/stack-manager/`, and make sure you have created postgis_password and geoserver_password in `inputs/secrets`, then execute the following
```
./stack.sh start flood-stack
```

For the committed docker-compose.yml, it is assumed that the datum file and connections file are already downloaded, more details on getting the files are described in the next section.
Execute the following to start the agent within the stack from the folder where this README is located
```
./stack.sh start flood-stack
```
Running this command will initialise the stations and start a scheduled update that downloads data from the API daily.

## Environment variables
These are all set in docker-compose.yml

Mandatory variables:
- GEOSERVER_WORKSPACE
- DATABASE (database name in PostGIS)
- LAYERNAME (Layer name in Geoserver, also the table name in PostGIS)
- ONTOP_FILE
- DATUM_FILE

Optional variables:
- DOWNLOAD_DATUM
- INSTANTIATE_CONNECTIONS
- CONNECTIONS_FILE

The following two variables work together, this adds the triple "<station> ems:hasObservationElevation ELEVATION" to each station
- DOWNLOAD_DATUM (if set to true, code will download stageScale for each station and write an output file to DATUM_FILE)
- DATUM_FILE (location of file containing DATUM data for each station)

You can either set DOWNLOAD_DATUM to true (making the code download the data, and instantiate them), or set it to false if you already have the file downloaded, and make sure it is available at the location specified in docker-compose.yml.

The following two variables work together:
- INSTANTIATE_CONNECTIONS (if set to true, adds triples <station1> <hasDownstreamStation> <station2> from the following file)
- CONNECTIONS_FILE (an output file generated by the python/river_sensors.py script)

## Logs
Logs are saved at `root/.jps/` by default, you can copy the logs into your local environment by using the following command
```
docker cp flood:/root/.jps .
```
or downloading it using VS Code's Docker plugin.

## Java commands
It is not necessary to run any of the following commands after spinning up the agent in the stack, spinning the agent up will take care of everything. The updating station commands might be useful to populate the database with historical data. 

Note that all the following commands need to be executed within the Docker container (e.g. using the open terminal option from Docker Desktop).

### Main entrypoint
The main entrypoint is the `LaunchScheduledUpdater` class, it is set as the main class in the manifest, i.e. running the `java -jar FloodAgent-2.0.0-SNAPSHOT.jar` command will launch this by default.

When launched, it will initialise the flood monitoring stations if they are not initialised, and start a scheduled task that runs once a day. The code will always download readings from the day before and upload the data to the time series tables in PostgreSQL.

### Initialisation
To initialise manually, it is possible to run the `InitialiseStations` class directly. It has a `main` function that does not need any inputs.

To run it on the command line:
```
java -cp FloodAgent-2.0.0-SNAPSHOT.jar uk.ac.cam.cares.jps.agent.flood.InitialiseStations
```

### Updating the stations
To manually add data from a specific date, run the `UpdateStations` class with a date as its input in ISO-8601 format, e.g. `2021-09-30`.

This class can be useful to populate the database with historical data, e.g. you can run the following in your script
```
java -cp FloodAgent-2.0.0-SNAPSHOT.jar uk.ac.cam.cares.jps.agent.flood.UpdateStations 2021-09-30
java -cp FloodAgent-2.0.0-SNAPSHOT.jar uk.ac.cam.cares.jps.agent.flood.UpdateStations 2021-10-01
java -cp FloodAgent-2.0.0-SNAPSHOT.jar uk.ac.cam.cares.jps.agent.flood.UpdateStations 2021-10-02
java -cp FloodAgent-2.0.0-SNAPSHOT.jar uk.ac.cam.cares.jps.agent.flood.UpdateStations 2021-10-03
```

### Resetting endpoints
Run the `ResetEndpoints` class to reset both the Blazegraph and Postgres database.
```
java -cp FloodAgent-2.0.0-SNAPSHOT.jar uk.ac.cam.cares.jps.agent.flood.ResetEndpoints
```

## Known issues with API
Executing get request to download the data for a particular data does not always yield the same set of data, the first download usually has missing data, downloading it again usually fixes this. There is probably some caching at the server.
